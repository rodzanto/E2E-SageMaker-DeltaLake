{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce535da8-cb8b-4d7b-a8f1-740d2a229f62",
   "metadata": {},
   "source": [
    "# From Delta Lake to Amazon SageMaker\n",
    "\n",
    "[Delta Lake](https://delta.io/) is a common open-source framework used for storing data in Lakehouse architectures.\n",
    "\n",
    "In this sample we demonstrate how to integrate Delta Tables with Amazon SageMaker for performing data exploration, ingestion, processing, training, and hosting for Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 1 - Data Exploration and Visualization\n",
    "\n",
    "***Use Kernel: SparkMagic (PySpark) for running this notebook***\n",
    "\n",
    "In this notebook, we will perform some Exploratory Data Analysis (EDA) over our Delta Tables with the two connection methods explained in the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708acb00-a01e-4f68-a2e2-be6b3a30aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.107.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51406204-684a-475c-9648-a4c189e4e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73e2b62-5677-41c5-ae84-ba43fac6269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-eu-west-1-889960878219\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "print('Default bucket: '+bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002e4c2-dcc6-4012-9f9f-a190920dd41e",
   "metadata": {},
   "source": [
    "----\n",
    "### Option 1: Reading from a Delta Table stored in Amazon S3 via Spark Session and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af9c9f6-abcd-4cb9-93ff-9ebc8ab629e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db58041-0426-41ad-ae0d-6fa4565889a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages: io.delta:delta-core_2.12:1.1.0,org.apache.hadoop:hadoop-aws:3.2.2\n"
     ]
    }
   ],
   "source": [
    "# Build list of packages entries using Maven coordinates (groupId:artifactId:version)\n",
    "pkg_list = []\n",
    "pkg_list.append(\"io.delta:delta-core_2.12:1.1.0\")\n",
    "pkg_list.append(\"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "\n",
    "packages=(\",\".join(pkg_list))\n",
    "print('packages: '+packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6a36a2-e157-4ca6-b5d8-9800ba6ce0fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1dcb8ee3-30c5-465a-abb4-807835d16ea3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 708ms :: artifacts dl 96ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1dcb8ee3-30c5-465a-abb4-807835d16ea3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/30ms)\n",
      "22/08/31 13:16:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/31 13:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark via builder\n",
    "# Note: we use the `ContainerCredentialsProvider` to give us access to underlying IAM role permissions\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PySparkApp\") \n",
    "    .config(\"spark.jars.packages\", packages) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .config(\"fs.s3a.aws.credentials.provider\",'com.amazonaws.auth.ContainerCredentialsProvider') \n",
    "    .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print('Spark version: '+str(sc.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130b6898-288c-4501-b9aa-8188b3c593d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/\n"
     ]
    }
   ],
   "source": [
    "s3a_delta_table_uri=f's3a://{bucket}/delta_to_sagemaker/delta_format/'\n",
    "print(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579397e4-f051-4e27-ba75-f99cc3feeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL command: SELECT * FROM delta.`s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/` ORDER BY medv\n"
     ]
    }
   ],
   "source": [
    "# Create SQL command inserting the S3 path location\n",
    "\n",
    "sql_cmd = f'SELECT * FROM delta.`{s3a_delta_table_uri}` ORDER BY medv'\n",
    "print(f'SQL command: {sql_cmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36012f7-91b1-4911-8237-b97f1a2e0dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/31 13:16:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 2:====>                                                     (4 + 2) / 50]\r"
     ]
    }
   ],
   "source": [
    "# Execute SQL command which returns dataframe\n",
    "\n",
    "sql_results = spark.sql(sql_cmd)\n",
    "print(type(sql_results))\n",
    "\n",
    "sql_results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11787bcb-9b2d-43c3-be0f-6b17fb1d55ee",
   "metadata": {},
   "source": [
    "(TBC)....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52f4dc-bdfd-4cb6-8912-7b838fc3f1c7",
   "metadata": {},
   "source": [
    "----\n",
    "### Option 2: Reading from an external Delta Table via Delta Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2275c6-f10c-45ef-890f-13a7f2c25dd0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile_file = \"https://raw.githubusercontent.com/delta-io/delta-sharing/main/examples/open-datasets.share\"\n",
    "!wget {profile_file} -P ./notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada9e04-828d-4331-8387-da865a772b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./notebooks/open-datasets.share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17c1df-c331-430d-8912-a91e69750711",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_profile_file_url = sagemaker.Session().upload_data(\n",
    "    \"./notebooks/open-datasets.share\", bucket=bucket, key_prefix=\"delta_to_sagemaker/delta_sharing/profile\"\n",
    ")\n",
    "\n",
    "print(sample_profile_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b3083-1d9a-4b6c-a3fd-64a7eaa50783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SharingClient\n",
    "import delta_sharing\n",
    "\n",
    "client = delta_sharing.SharingClient(sample_profile_file_url)\n",
    "table_url = profile_file + \"#delta_sharing.default.boston-housing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6441239-5b13-4942-a845-cb174a20ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table as a Pandas DataFrame\n",
    "print('Loading boston-housing table from Delta Lake')\n",
    "train_data = delta_sharing.load_as_spark(table_url)\n",
    "print(f'Train data shape: {train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab76f8c-5b93-4c4e-84b7-3db3b94c7ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58707a6-1705-47b7-9795-4238cf0e333e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f5fbf-e90c-49e2-8f97-0c13ae257af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfef2f-e366-4b77-9ec1-fcc89d0e3107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
