{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce535da8-cb8b-4d7b-a8f1-740d2a229f62",
   "metadata": {},
   "source": [
    "# From Delta Lake to Amazon SageMaker\n",
    "\n",
    "## 1 - Data Exploration and Visualization\n",
    "\n",
    "In this notebook, we will..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708acb00-a01e-4f68-a2e2-be6b3a30aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.107.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a696fa5e-92ac-4ba3-824c-a44e69529dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db58041-0426-41ad-ae0d-6fa4565889a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages: io.delta:delta-core_2.12:1.1.0,org.apache.hadoop:hadoop-aws:3.2.2\n"
     ]
    }
   ],
   "source": [
    "# Build list of packages entries using Maven coordinates (groupId:artifactId:version)\n",
    "pkg_list = []\n",
    "pkg_list.append(\"io.delta:delta-core_2.12:1.1.0\")\n",
    "pkg_list.append(\"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "\n",
    "packages=(\",\".join(pkg_list))\n",
    "print('packages: '+packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6a36a2-e157-4ca6-b5d8-9800ba6ce0fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-24c10868-db4e-47e7-88f8-33b9875e84c5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 699ms :: artifacts dl 62ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-24c10868-db4e-47e7-88f8-33b9875e84c5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/17ms)\n",
      "22/08/31 11:56:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/31 11:56:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark via builder\n",
    "# Note: we use the `ContainerCredentialsProvider` to give us access to underlying IAM role permissions\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PySparkApp\") \n",
    "    .config(\"spark.jars.packages\", packages) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .config(\"fs.s3a.aws.credentials.provider\",'com.amazonaws.auth.ContainerCredentialsProvider') \n",
    "    .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print('Spark version: '+str(sc.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73e2b62-5677-41c5-ae84-ba43fac6269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-eu-west-1-889960878219\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# S3 bucket for saving processing job outputs\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "print('Default bucket: '+bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130b6898-288c-4501-b9aa-8188b3c593d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/\n"
     ]
    }
   ],
   "source": [
    "s3a_delta_table_uri=f's3a://{bucket}/delta_to_sagemaker/delta_format/'\n",
    "print(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "579397e4-f051-4e27-ba75-f99cc3feeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL command: SELECT * FROM delta.`s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/` ORDER BY medv\n"
     ]
    }
   ],
   "source": [
    "# Create SQL command inserting the S3 path location\n",
    "\n",
    "sql_cmd = f'SELECT * FROM delta.`{s3a_delta_table_uri}` ORDER BY medv'\n",
    "print(f'SQL command: {sql_cmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36012f7-91b1-4911-8237-b97f1a2e0dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/31 11:56:27 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|   crim| zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|\n",
      "+-------+---+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|14.3337|  0| 18.1|   0|  0.7| 4.88| 100|1.5895| 24|666|   20.2|372.92|30.62|10.2|\n",
      "|12.2472|  0| 18.1|   0|0.584|5.837|59.7|1.9976| 24|666|   20.2| 24.65|15.69|10.2|\n",
      "|17.8667|  0| 18.1|   0|0.671|6.223| 100|1.3861| 24|666|   20.2|393.74|21.78|10.2|\n",
      "|88.9762|  0| 18.1|   0|0.671|6.968|91.9|1.4165| 24|666|   20.2| 396.9|17.21|10.4|\n",
      "|25.9406|  0| 18.1|   0|0.679|5.304|89.1|1.6475| 24|666|   20.2|127.36|26.64|10.4|\n",
      "|22.0511|  0| 18.1|   0| 0.74|5.818|92.4|1.8662| 24|666|   20.2|391.45|22.11|10.5|\n",
      "|24.3938|  0| 18.1|   0|  0.7|4.652| 100|1.4672| 24|666|   20.2| 396.9|28.28|10.5|\n",
      "|12.8023|  0| 18.1|   0| 0.74|5.854|96.6|1.8956| 24|666|   20.2|240.52|23.79|10.8|\n",
      "|15.8744|  0| 18.1|   0|0.671|6.545|99.1|1.5192| 24|666|   20.2| 396.9|21.08|10.9|\n",
      "|37.6619|  0| 18.1|   0|0.679|6.202|78.7|1.8629| 24|666|   20.2| 18.82|14.52|10.9|\n",
      "+-------+---+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Execute SQL command which returns dataframe\n",
    "\n",
    "sql_results = spark.sql(sql_cmd)\n",
    "print(type(sql_results))\n",
    "\n",
    "sql_results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a762142-3613-4222-8497-97598b82772c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Using cached pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
      "Requirement already satisfied: joblib~=1.1.0 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (1.1.0)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (1.1.5)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (1.19.5)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (5.4.1)\n",
      "Collecting phik>=0.11.1\n",
      "  Using cached phik-0.12.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (696 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (3.1.2)\n",
      "Collecting visions[type_image_path]==0.7.4\n",
      "  Using cached visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "Collecting missingno>=0.4.2\n",
      "  Using cached missingno-0.5.1-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (0.11.1)\n",
      "Collecting multimethod>=1.4\n",
      "  Using cached multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (1.7.1)\n",
      "Collecting pydantic>=1.8.1\n",
      "  Using cached pydantic-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.2.0 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (2.27.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (3.4.3)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (4.62.3)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in /opt/conda/lib/python3.8/site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.8/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (20.3.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /opt/conda/lib/python3.8/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.6.3)\n",
      "Collecting imagehash\n",
      "  Using cached ImageHash-4.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (9.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2021.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.8/site-packages (from pydantic>=1.8.1->pandas-profiling) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.24.0->pandas-profiling) (1.26.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=3.2.0->pandas-profiling) (1.16.0)\n",
      "Requirement already satisfied: PyWavelets in /opt/conda/lib/python3.8/site-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.1.1)\n",
      "Installing collected packages: pydantic, multimethod, imagehash, visions, phik, missingno, pandas-profiling\n",
      "Successfully installed imagehash-4.2.1 missingno-0.5.1 multimethod-1.8 pandas-profiling-3.2.0 phik-0.12.2 pydantic-1.10.1 visions-0.7.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e4957-5503-4854-89a7-5c559558cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573ccb2f6a374ed0852c3b5953ab55b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "df = sql_results.toPandas()\n",
    "report = ProfileReport(df)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375b12d-7079-401a-8f3f-c785a426766c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
